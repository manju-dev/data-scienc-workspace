{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK features and Spelling Recommender\n",
    "\n",
    "Use nltk to explore the Herman Melville novel Moby Dick and create Spelling Recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download books from nltk\n",
    "nltk.download('book', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore NLTK Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenized novel in nltk.Text format\n",
    "moby = text1\n",
    "type(moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens (words and punctuation symbols)\n",
    "len(moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique tokens (unique words and punctuation)\n",
    "len(set(moby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15363"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizing the verbs\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w,'v') for w in moby]\n",
    "\n",
    "# unique tokens after lemmatizing\n",
    "len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07406285585022564"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical diversity of the text (i.e. ratio of unique tokens to the total number of tokens)\n",
    "len(set(moby)) / len(moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.455488288813315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of tokens is 'whale'or 'Whale'\n",
    "((moby.count('whale') + moby.count('Whale')) / len(moby)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982),\n",
       " (\"'\", 2684),\n",
       " ('-', 2552),\n",
       " ('his', 2459),\n",
       " ('it', 2209),\n",
       " ('I', 2124),\n",
       " ('s', 1739),\n",
       " ('is', 1695),\n",
       " ('he', 1661),\n",
       " ('with', 1659),\n",
       " ('was', 1632)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 most frequently occurring (unique) tokens in the text with their frequency\n",
    "from nltk import FreqDist\n",
    "    \n",
    "mobyDist = FreqDist(moby)\n",
    "sorted_x = sorted(mobyDist.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_x[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with length of greater than 5 and frequency of more than 150\n",
    "freqWords = [w for w in mobyDist.keys() if len(w) > 5 and mobyDist[w] > 150]\n",
    "sorted(freqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uninterpenetratingly', 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longest word in moby\n",
    "longestWord = ''\n",
    "for word in moby:\n",
    "    if len(word) > len(longestWord):\n",
    "        longestWord = word\n",
    "(longestWord, len(longestWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13721, 'the'),\n",
       " (6536, 'of'),\n",
       " (6024, 'and'),\n",
       " (4569, 'a'),\n",
       " (4542, 'to'),\n",
       " (3916, 'in'),\n",
       " (2982, 'that'),\n",
       " (2459, 'his'),\n",
       " (2209, 'it'),\n",
       " (2124, 'I')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words with frequency of more than 2000\n",
    "freqWords = [(mobyDist[w], w) for w in mobyDist.keys() if w.isalpha() and mobyDist[w] > 2000]\n",
    "sorted(freqWords, key=lambda item: item[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 36099), ('IN', 28964), ('DT', 25826), ('JJ', 18811), (',', 18713)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 most frequent parts of speech\n",
    "posTagList = nltk.pos_tag(moby)\n",
    "    \n",
    "result = {}\n",
    "for item in posTagList:\n",
    "    if item[1] in result:\n",
    "        result[item[1]] += 1\n",
    "    else:\n",
    "        result[item[1]] = 1\n",
    "            \n",
    "sorted_x = sorted(result.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Recommender\n",
    "\n",
    "Create different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled words.\n",
    "\n",
    "For every misspelled word, the recommender will find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Different recommenders will use a different distance measuresbased on [1] and [2]\n",
    "\n",
    "[1] [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index)\n",
    "\n",
    "[2] [Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard distance on the trigrams of the two words.\n",
    "def jaccard_trigram_spellrec(entries):\n",
    "    result = []\n",
    "    for word in entries:\n",
    "        closeMatch = (0, 1)\n",
    "        for item in correct_spellings:\n",
    "            if item[0] == word[0]:\n",
    "                distance = nltk.jaccard_distance(set(nltk.ngrams(word, n=3)), set(nltk.ngrams(item, n=3)))\n",
    "                if distance < closeMatch[1]:\n",
    "                    closeMatch = (item, distance)\n",
    "        result.append(closeMatch[0])\n",
    "    return result\n",
    "    \n",
    "jaccard_trigram_spellrec(['cormulent', 'incendenece', 'validrate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard distance on the 4-grams of the two words.\n",
    "def jaccard_4gram_spellrec(entries):\n",
    "    result = []\n",
    "    for word in entries:\n",
    "        closeMatch = (0, 1)\n",
    "        for item in correct_spellings:\n",
    "            if item[0] == word[0]:\n",
    "                distance = nltk.jaccard_distance(set(nltk.ngrams(word, n=4)), set(nltk.ngrams(item, n=4)))\n",
    "                if distance < closeMatch[1]:\n",
    "                    closeMatch = (item, distance)\n",
    "        result.append(closeMatch[0])\n",
    "    return result\n",
    "    \n",
    "jaccard_4gram_spellrec(['cormulent', 'incendenece', 'validrate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edit distance on the two words with transpositions.\n",
    "def edit_distance_spellrec(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    result = []\n",
    "    for word in entries:\n",
    "        closeMatch = (None, None)\n",
    "        for item in correct_spellings:\n",
    "            if item[0] == word[0]:\n",
    "                distance = nltk.edit_distance(word, item)\n",
    "                if closeMatch[1] == None:\n",
    "                    closeMatch = (item, distance)\n",
    "                elif distance < closeMatch[1]:\n",
    "                    closeMatch = (item, distance)\n",
    "        result.append(closeMatch[0])    \n",
    "    return result\n",
    "    \n",
    "edit_distance_spellrec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
